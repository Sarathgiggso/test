apiVersion: v1
data:
  capacity-schedular.xml: |
    <?xml version="1.0"?>
    <!--
      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at
        http://www.apache.org/licenses/LICENSE-2.0
      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License. See accompanying LICENSE file.
    -->
    <configuration>

        <property>
            <name>yarn.scheduler.capacity.maximum-applications</name>
            <value>10000</value>
            <description>
                Maximum number of applications that can be pending and running.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
            <value>1</value>
            <description>
                Maximum percent of resources in the cluster which can be used to run
                application masters i.e. controls number of concurrent running
                applications.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.root.queues</name>
            <value>default</value>
            <description>
                The queues at the this level (root is the root queue).
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.root.default.capacity</name>
            <value>100</value>
            <description>Default queue target capacity.</description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
            <value>100</value>
            <description>
                The maximum capacity of the default queue.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.root.default.state</name>
            <value>RUNNING</value>
            <description>
                The state of the default queue. State can be one of RUNNING or STOPPED.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
            <value>*</value>
            <description>
                The ACL of who can submit jobs to the default queue.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
            <value>1</value>
            <description>
                Default queue user limit a percentage from 0.0 to 1.0.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
            <value>*</value>
            <description>
                The ACL of who can administer jobs on the default queue.
            </description>
        </property>

        <property>
            <name>yarn.scheduler.capacity.node-locality-delay</name>
            <value>-1</value>
            <description>
                Number of missed scheduling opportunities after which the CapacityScheduler
                attempts to schedule rack-local containers.
                Typically this should be set to number of racks in the cluster, this
                feature is disabled by default, set to -1.
            </description>
        </property>

    </configuration>
  core-site.xml: "<?xml version=\"1.0\"?>\n<!--\n  Licensed to the Apache Software
    Foundation (ASF) under one or more\n  contributor license agreements.  See the
    NOTICE file distributed with\n  this work for additional information regarding
    copyright ownership.\n  The ASF licenses this file to You under the Apache License,
    Version 2.0\n  (the \"License\"); you may not use this file except in compliance
    with\n  the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n
    \ Unless required by applicable law or agreed to in writing, software\n  distributed
    under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES
    OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the
    specific language governing permissions and\n  limitations under the License.\n-->\n<configuration>\n\n
    \   <!-- Hive impersonation -->\n    <property>\n        <name>hadoop.proxyuser.hive.hosts</name>\n
    \       <value>*</value>\n    </property>\n\n    <property>\n        <name>hadoop.proxyuser.hive.groups</name>\n
    \       <value>*</value>\n    </property>\n\n    <property>\n        <name>fs.defaultFS</name>\n
    \       <value>hdfs://hadoop-master:9000</value>\n    </property>\n\n    <property>\n
    \       <name>fs.s3.awsAccessKeyId</name>\n        <value>AKIAJL3AXLX5P5O5FYTA</value>\n
    \   </property>\n\n    <property>\n        <name>fs.s3.awsSecretAccessKey</name>\n
    \       <value>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</value>\n    </property>\n\n
    \   <property>\n        <name>fs.s3a.access.key</name>\n        <value>AKIAJL3AXLX5P5O5FYTA</value>\n
    \   </property>\n\n    <property>\n        <name>fs.s3a.secret.key</name>\n        <value>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</value>\n
    \   </property>\n\n    <property>\n        <name>fs.s3a.endpoint</name>\n        <value>https://gigsstackoverflowdata.s3.ap-south-1.amazonaws.com</value>\n
    \   </property>\n\n    <property>\n      <name>fs.azure.account.key.${env.STORAGE_ACCOUNT}.blob.core.windows.net</name>\n
    \     <value>${env.AZURE_ACCESS_KEY}</value>\n
    \   </property>\n    \n</configuration>\n"
  core-site.xml.wasb-template: |+
    <?xml version="1.0"?>
    <!--
      Licensed to the Apache Software Foundation (ASF) under one or more
      contributor license agreements.  See the NOTICE file distributed with
      this work for additional information regarding copyright ownership.
      The ASF licenses this file to You under the Apache License, Version 2.0
      (the "License"); you may not use this file except in compliance with
      the License.  You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
    -->
    <configuration>

        <!-- Hive impersonation -->
        <property>
            <name>hadoop.proxyuser.hive.hosts</name>
            <value>*</value>
        </property>

        <property>
            <name>hadoop.proxyuser.hive.groups</name>
            <value>*</value>
        </property>

        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://hadoop-master:9000</value>
        </property>

        <property>
          <name>fs.azure.account.key.${env.STORAGE_ACCOUNT}.blob.core.windows.net</name>
          <value>${env.AZURE_ACCESS_KEY}</value>
        </property>   
    </configuration>

  hadoop-env.sh: |
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    # Set Hadoop-specific environment variables here.
    # Forcing YARN-based mapreduce implementaion.
    # Make sure to comment out if you want to go back to the default or
    # if you want this to be tweakable on a per-user basis
    # export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce

    # The maximum amount of heap to use, in MB. Default is 1000.
    export HADOOP_HEAPSIZE=256

    # Extra Java runtime options.  Empty by default.
    export HADOOP_NAMENODE_OPTS="$HADOOP_NAMENODE_OPTS -Xmx512m"
    export YARN_OPTS="$YARN_OPTS -Xmx256m"
  hadoop-metrics2.properties: "# syntax: [prefix].[source|sink].[instance].[options]\n#
    See javadoc of package-info.java for org.apache.hadoop.metrics2 for details\n\n*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink\n#
    default sampling period, in seconds\n*.period=10\n\n# The namenode-metrics.out
    will contain metrics from all context\n#namenode.sink.file.filename=namenode-metrics.out\n#
    Specifying a special sampling period for namenode:\n#namenode.sink.*.period=8\n\n#datanode.sink.file.filename=datanode-metrics.out\n\n#resourcemanager.sink.file.filename=resourcemanager-metrics.out\n\n#nodemanager.sink.file.filename=nodemanager-metrics.out\n\n#mrappmaster.sink.file.filename=mrappmaster-metrics.out\n\n#jobhistoryserver.sink.file.filename=jobhistoryserver-metrics.out\n\n#
    the following example split metrics of different\n# context to different sinks
    (in this case files)\n#nodemanager.sink.file_jvm.class=org.apache.hadoop.metrics2.sink.FileSink\n#nodemanager.sink.file_jvm.context=jvm\n#nodemanager.sink.file_jvm.filename=nodemanager-jvm-metrics.out\n#nodemanager.sink.file_mapred.class=org.apache.hadoop.metrics2.sink.FileSink\n#nodemanager.sink.file_mapred.context=mapred\n#nodemanager.sink.file_mapred.filename=nodemanager-mapred-metrics.out\n\n#\n#
    Below are for sending metrics to Ganglia\n#\n# for Ganglia 3.0 support\n# *.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30\n#\n#
    for Ganglia 3.1 support\n# *.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31\n\n#
    *.sink.ganglia.period=10\n\n# default for supportsparse is false\n# *.sink.ganglia.supportsparse=true\n\n#*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both\n#*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40\n\n#
    Tag values to use for the ganglia prefix. If not defined no tags are used.\n#
    If '*' all tags are used. If specifiying multiple tags separate them with \n#
    commas. Note that the last segment of the property name is the context name.\n#\n#
    A typical use of tags is separating the metrics by the HDFS rpc port\n# and HDFS
    service rpc port.\n# For example:\n#   With following HDFS configuration:\n#       dfs.namenode.rpc-address
    is set as namenodeAddress:9110\n#       dfs.namenode.servicerpc-address is set
    as namenodeAddress:9111\n#   If no tags are used, following metric would be gathered:\n#
    \      rpc.rpc.NumOpenConnections\n#   If using \"*.sink.ganglia.tagsForPrefix.rpc=port\",\n#
    \  following metrics would be gathered:\n#       rpc.rpc.port=9110.NumOpenConnections\n#
    \      rpc.rpc.port=9111.NumOpenConnections\n#\n#*.sink.ganglia.tagsForPrefix.jvm=ProcessName\n#*.sink.ganglia.tagsForPrefix.dfs=HAState,IsOutOfSync\n#*.sink.ganglia.tagsForPrefix.rpc=port\n#*.sink.ganglia.tagsForPrefix.rpcdetailed=port\n#*.sink.ganglia.tagsForPrefix.metricssystem=*\n#*.sink.ganglia.tagsForPrefix.ugi=*\n#*.sink.ganglia.tagsForPrefix.mapred=\n\n#namenode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649\n\n#datanode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649\n\n#resourcemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649\n\n#nodemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649\n\n#mrappmaster.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649\n\n#jobhistoryserver.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649\ns3a-file-system.sink.my-metrics-config.class=org.apache.hadoop.metrics2.sink.FileSink\ns3a-file-system.sink.my-metrics-config.filename=/var/log/hadoop-yarn/s3a-metrics.out\n*.period=10\n"
  hadoop-put.sh: |+
    #!/bin/bash

    set -xeuo pipefail

    # Hadoop 3 without -d (don't create _COPYING_ temporary file) requires additional S3 permissions
    # Hadoop 2 doesn't have '-d' switch
    hadoop fs -put -f -d "$@" ||
    hadoop fs -put -f "$@"

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <!--
      ~ Licensed under the Apache License, Version 2.0 (the "License");
      ~ you may not use this file except in compliance with the License.
      ~ You may obtain a copy of the License at
      ~
      ~     http://www.apache.org/licenses/LICENSE-2.0
      ~
      ~ Unless required by applicable law or agreed to in writing, software
      ~ distributed under the License is distributed on an "AS IS" BASIS,
      ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      ~ See the License for the specific language governing permissions and
      ~ limitations under the License.
      ~
      -->
    <configuration>

        <property>
            <name>dfs.namenode.name.dir</name>
            <value>/var/lib/hadoop-hdfs/cache/name/</value>
        </property>

        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/var/lib/hadoop-hdfs/cache/data/</value>
        </property>

        <property>
            <name>fs.viewfs.mounttable.hadoop-viewfs.link./default</name>
            <value>hdfs://hadoop-master:9000/user/hive/warehouse</value>
        </property>

        <!-- This property explicitly forbids datanode to enter safe mode which results in 30 s penalty on environment startup -->
        <property>
            <name>dfs.safemode.threshold.pct</name>
            <value>0</value>
        </property>

    </configuration>
  log4j.properties: |
    log4j.rootLogger=INFO,CONSOLE
    log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
    log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
    log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n
  mapred-site.xml: |
    <?xml version="1.0"?>
    <!--
      Licensed to the Apache Software Foundation (ASF) under one or more
      contributor license agreements.  See the NOTICE file distributed with
      this work for additional information regarding copyright ownership.
      The ASF licenses this file to You under the Apache License, Version 2.0
      (the "License"); you may not use this file except in compliance with
      the License.  You may obtain a copy of the License at
          http://www.apache.org/licenses/LICENSE-2.0
      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
    -->
    <configuration>

        <property>
            <name>mapred.job.tracker</name>
            <value>hadoop-master:8021</value>
        </property>

        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>

        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>hadoop-master:10020</value>
        </property>

        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>hadoop-master:19888</value>
        </property>

        <property>
            <description>To set the value of tmp directory for map and reduce tasks.</description>
            <name>mapreduce.task.tmp.dir</name>
            <value>/var/lib/hadoop-mapreduce/cache/${user.name}/tasks</value>
        </property>

    </configuration>
  tez-site.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at
        http://www.apache.org/licenses/LICENSE-2.0
      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License. See accompanying LICENSE file.
    -->

    <configuration>
        <property>
            <name>tez.lib.uris.ignore</name>
            <value>false</value>
        </property>
        <property>
            <name>tez.lib.uris</name>
            <value>file:///usr/hdp/current/tez-client/lib/tez.tar.gz</value>
        </property>
        <property>
            <name>tez.am.mode.session</name>
            <value>false</value>
        </property>
        <property>
            <name>tez.am.acl.enabled</name>
            <value>false</value>
        </property>
        <property>
            <name>tez.am.log.level</name>
            <value>WARN</value>
        </property>
        <property>
            <name>tez.task.log.level</name>
            <value>WARN</value>
        </property>
        <property>
            <name>tez.runtime.io.sort.mb</name>
            <value>8</value>
        </property>
        <property>
            <name>tez.am.max.app.attempts</name>
            <value>1</value>
        </property>
        <property>
            <name>tez.am.task.max.failed.attempts</name>
            <value>1</value>
        </property>
        <property>
            <name>tez.shuffle-vertex-manager.min-src-fraction</name>
            <value>0.10</value>
        </property>
        <property>
            <name>tez.shuffle-vertex-manager.max-src-fraction</name>
            <value>1.00</value>
        </property>
        <property>
            <name>tez.am.launch.cmd-opts</name>
            <value>-server -Djava.net.preferIPv4Stack=true -XX:+UseParallelGC -Dhadoop.metrics.log.level=WARN</value>
        </property>
        <property>
            <name>tez.am.resource.memory.mb</name>
            <value>512</value>
        </property>
        <property>
            <name>tez.task.launch.cmd-opts</name>
            <value>-server -Djava.net.preferIPv4Stack=true -XX:+UseParallelGC -Dhadoop.metrics.log.level=WARN</value>
        </property>
        <property>
            <name>tez.task.resource.memory.mb</name>
            <value>512</value>
        </property>
        <property>
            <name>tez.task.resource.cpu.vcores</name>
            <value>1</value>
        </property>
        <property>
            <name>tez.runtime.sort.threads</name>
            <value>1</value>
        </property>
        <property>
            <name>tez.runtime.io.sort.factor</name>
            <value>100</value>
        </property>
        <property>
            <name>tez.runtime.shuffle.memory-to-memory.enable</name>
            <value>false</value>
        </property>
        <property>
            <name>tez.runtime.optimize.local.fetch</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.tez.container.size</name>
            <value>2048</value>
        </property>
    </configuration>
  yarn-site.xml: |+
    <?xml version="1.0"?>
    <!--
      Licensed to the Apache Software Foundation (ASF) under one or more
      contributor license agreements.  See the NOTICE file distributed with
      this work for additional information regarding copyright ownership.
      The ASF licenses this file to You under the Apache License, Version 2.0
      (the "License"); you may not use this file except in compliance with
      the License.  You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
    -->

    <configuration>

        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>

        <property>
            <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        </property>

        <property>
            <name>yarn.log-aggregation-enable</name>
            <value>true</value>
        </property>

        <property>
            <name>yarn.dispatcher.exit-on-error</name>
            <value>true</value>
        </property>

        <property>
            <description>List of directories to store localized files in.</description>
            <name>yarn.nodemanager.local-dirs</name>
            <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
        </property>

        <property>
            <description>Where to store container logs.</description>
            <name>yarn.nodemanager.log-dirs</name>
            <value>/var/log/hadoop-yarn/containers</value>
        </property>

        <property>
            <description>Where to aggregate logs to.</description>
            <name>yarn.nodemanager.remote-app-log-dir</name>
            <value>/var/log/hadoop-yarn/apps</value>
        </property>

        <property>
            <description>Classpath for typical applications.</description>
            <name>yarn.application.classpath</name>
            <value>
                /etc/hadoop/conf,
                /usr/hdp/current/hadoop-client/*,
                /usr/hdp/current/hadoop-client/lib/*,
                /usr/hdp/current/hadoop-hdfs-client/*,
                /usr/hdp/current/hadoop-hdfs-client/lib/*,
                /usr/hdp/current/hadoop-yarn-client/*,
                /usr/hdp/current/hadoop-yarn-client/lib/*,
                /usr/hdp/current/hadoop-mapreduce-client/*,
                /usr/hdp/current/hadoop-mapreduce-client/lib/*
            </value>
        </property>

        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>hadoop-master</value>
        </property>

        <property>
            <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
            <value>100</value>
        </property>

        <property>
            <name>yarn.nodemanager.pmem-check-enabled</name>
            <value>false</value>
        </property>

        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>

        <property>
            <name>yarn.nodemanager.resource.memory.enforced</name>
            <value>false</value>
        </property>

        <property>
            <name>yarn.nodemanager.elastic-memory-control.enabled</name>
            <value>false</value>
        </property>

        <property>
            <name>yarn.log.server.url</name>
            <value>http://hadoop-master:19888/jobhistory/logs</value>
        </property>

        <!-- Minimum container memory, default value is 1024 which is too high for the test queries -->
        <property>
            <name>yarn.scheduler.minimum-allocation-mb</name>
            <value>256</value>
            <description>The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this won't take effect, and the specified value will get allocated at minimum.</description>
        </property>

    </configuration>

kind: ConfigMap
metadata:
  creationTimestamp: null
  name: metastore-cfg
